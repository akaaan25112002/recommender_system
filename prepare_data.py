import os
import pandas as pd
from utils import filter_vietnamese_words
from underthesea import word_tokenize
import gdown

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, 'Data')
os.makedirs(DATA_DIR, exist_ok=True)

# File paths
CLEANED_PRODUCTS_FILE = os.path.join(DATA_DIR, 'cleaned_products.csv')
CLEANED_RATINGS_FILE = os.path.join(DATA_DIR, 'cleaned_ratings.csv')
RAW_PRODUCTS_FILE = os.path.join(DATA_DIR, 'Products_ThoiTrangNam_raw.csv')
RAW_RATINGS_FILE = os.path.join(DATA_DIR, 'Products_ThoiTrangNam_rating_raw.csv')

# Google Drive File IDs (t√πy b·∫°n thay v√†o)
CLEANED_PRODUCTS_ID = "1ABCxyz_cleaned_products_ID"
CLEANED_RATINGS_ID = "1DEFxyz_cleaned_ratings_ID"
RAW_PRODUCTS_ID = "1kMQ6Fk__epxgcBQADdGSf2OdUM5YZmgR"
RAW_RATINGS_ID = "10mS7UAzMf-VtHlvgiuSQYJ5L22LAzpdH"

def download_file(file_id, output_path, desc):
    print(f"üîΩ ƒêang t·∫£i {desc} t·ª´ Google Drive...")
    gdown.download(f"https://drive.google.com/uc?id={file_id}", output_path, quiet=False)

def load_and_prepare_data():
    # ∆Øu ti√™n d√πng file ƒë√£ cleaned n·∫øu c√≥
    if os.path.exists(CLEANED_PRODUCTS_FILE) and os.path.exists(CLEANED_RATINGS_FILE):
        print("‚úÖ ƒêang load d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c cleaned...")
        products = pd.read_csv(CLEANED_PRODUCTS_FILE)
        ratings = pd.read_csv(CLEANED_RATINGS_FILE)
    else:
        # N·∫øu ch∆∞a c√≥ th√¨ x·ª≠ l√Ω l·∫°i t·ª´ file raw
        if not os.path.exists(RAW_PRODUCTS_FILE):
            download_file(RAW_PRODUCTS_ID, RAW_PRODUCTS_FILE, "Products raw")
        if not os.path.exists(RAW_RATINGS_FILE):
            download_file(RAW_RATINGS_ID, RAW_RATINGS_FILE, "Ratings raw")

        products = pd.read_csv(RAW_PRODUCTS_FILE)
        ratings = pd.read_csv(RAW_RATINGS_FILE, sep="\t")

        # X·ª≠ l√Ω d·ªØ li·ªáu nh∆∞ tr∆∞·ªõc
        products['image'] = products['image'].fillna('No image available')
        products['description'] = products['description'].fillna('No description available')

        ratings = ratings.drop_duplicates()
        ratings_avg = ratings.groupby(['user_id', 'product_id'])['rating'].mean().reset_index()
        ratings_avg['rating'] = ratings_avg['rating'].round(0)
        ratings = ratings_avg[ratings_avg['product_id'].isin(products['product_id'])]

        data = products[['product_id', 'product_name', 'sub_category', 'price', 'rating', 'image', 'description']].copy()
        data['description'] = data['description'].str.replace('Danh M·ª•c\nShopee\nTh·ªùi Trang Nam\n', '', regex=False)
        data['description'] = data['description'].str.replace('\n', ' ')
        data['description_clean'] = data['description'].apply(filter_vietnamese_words)
        data['Content'] = data['product_name'] + ' ' + data['description_clean'].apply(lambda x: ' '.join(x.split()[:200]))
        data['tokens'] = data['description_clean'].apply(lambda x: word_tokenize(str(x), format="text").split())

        # Save cleaned
        data.to_csv(CLEANED_PRODUCTS_FILE, index=False)
        ratings.to_csv(CLEANED_RATINGS_FILE, index=False)

        products = data  # d√πng cleaned version ƒë·ªÉ ƒë·ªìng nh·∫•t

    final_data = pd.merge(ratings, products, how='inner', on='product_id')
    print("‚úÖ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng!")

    return products, ratings, final_data

if __name__ == "__main__":
    products, ratings, final_data = load_and_prepare_data()